{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-Q1 PLN Notebook 36.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXaeYrp0rDHOsZwReJiNSx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2021.QS-PLN/blob/main/2021_Q1_PLN_Notebook_36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alARqIkOqgJi"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2021.Q1]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Zy-WgNqlgX"
      },
      "source": [
        "### **Reconhecimento de Entidades Nomeadas** \n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpefzFXhqqPI"
      },
      "source": [
        "Existem diversas aplicações que podem fazer uso dessa técnica:\n",
        "\n",
        "* **Resumo de grandes documentos**: um sistema que usa NER poderia aprender a resumir textos grandes em pequenos a partir do conhecimento de entidades relevantes.\n",
        "* **Categorização de *reviews***: ao aplicar NER em comentários de usuários em portais, pode se gerar categorias que resumem todo o texto.\n",
        "* **Chatbot de auto-atendimento**: diversos chatbots estão sendo construídos nos quais a base para seu funcionamento é a técnica de NER. Com o reconhecimento das entidades os bots tomam ações propícias conforme o usuário fala ou digita.\n",
        "* **Identificação e agendamento de compromissos**: usando NER um sistema consegue aprender o que é data, horário e uma cidade em um e-mail por exemplo. Com essas informações, o sistema poderia agendar um compromisso automaticamente na sua agenda.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oejJSPqvr2ko"
      },
      "source": [
        "Primeiramente precisamos fazer download de alguns modelos prontos. Esses modelos contêm informações sobre linguagens, vocabulários, vetores treinados, sintaxes e entidades.\n",
        "\n",
        ">\n",
        "Vamos inicar fazendo o download dos modelos para o idioma Português e Inglês:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q74ql5csGAQ",
        "outputId": "3915fd21-a11c-4954-cef7-783d0e9a29d0"
      },
      "source": [
        "!python -m spacy download pt\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pt_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 83.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp37-none-any.whl size=21186283 sha256=8cb856a19f137ab454fa812eaad2d653e3eb1e48ced7088f08af68e9f81fca4a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qbb8h2yv/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70-JdR3ssSIV"
      },
      "source": [
        "Em seguida, carregamos o modelo português e passamos uma sentença para o método **`nlp()`** para este iniciar o processamento do texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoADvermsked",
        "outputId": "75e0bc46-0d51-4e9b-ef62-88cf13a99b77"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "texto = nlp('Olá Mundo. Clássica frase no mundo da computação')\n",
        "texto"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Olá Mundo. Clássica frase no mundo da computação"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb6K8fnAtD7V",
        "outputId": "bf776307-df27-45da-fbb1-8021a2ecd06c"
      },
      "source": [
        "type(texto)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLTXx0fatMr-"
      },
      "source": [
        "Cada palavra da sentença para o spaCy é um token. Isso significa que esse objeto contém propriedades interessantes. Vejamos alguns exemplos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob7enn51tTtS",
        "outputId": "e0344766-bcfc-4865-db8d-75787db009fb"
      },
      "source": [
        "for token in texto:\n",
        "  print(token, token.idx, token.shape_, token.tag_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Olá 0 Xxx PROPN\n",
            "Mundo 4 Xxxxx PROPN\n",
            ". 9 . PU|@PU\n",
            "Clássica 11 Xxxxx PROP|F|S|@SUBJ>\n",
            "frase 20 xxxx <np-def>|N|F|S|@SUBJ>\n",
            "no 26 xx PRP|@N<\n",
            "mundo 29 xxxx <np-idf>|N|M|S|@P<\n",
            "da 35 xx PRP|@N<\n",
            "computação 38 xxxx <np-idf>|N|F|S|@P<\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le-bse2ytkF3"
      },
      "source": [
        "A propriedade **`.idx`** retorna o índice de inicio do token (palavra).\n",
        "\n",
        "A propriedade **`.shape`**_ mostra o formato da palavra, se esta começa com letra maiúscula tem uma forma diferente, por exemplo.\n",
        "\n",
        "Já a propriedade **`.tag_`** mostra informações gramáticais como ‘PROPN’ que significa ‘Proper Noun’, ou seja, substantivo próprio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emLXJy-wuBKW",
        "outputId": "bad1a047-7980-4b1d-9074-facff5685135"
      },
      "source": [
        "texto = nlp('O rato roeu a roupa do rei de Roma')\n",
        "\n",
        "for entidade in texto.ents:\n",
        "  print(entidade.text, entidade.label_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Roma LOC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Pj9dCZuoF_"
      },
      "source": [
        "texto = nlp('Maria está se mudando para Paris. No dia 25/12/2020 ela irá partir')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEDmHnnBvKps"
      },
      "source": [
        "Temos agora 3 entidades que queremos identificar, no caso a pessoa \"Maria\", a cidade de \"Paris\" e a data \"25/12/2020\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Yl1YgAL4vHys",
        "outputId": "da4497ac-83f9-4fd4-fee0-0d1f97fafae9"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(texto, style='ent', jupyter=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Maria\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " está se mudando para \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Paris\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ". No dia 25/12/2020 ela irá partir</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hNwHRoXnwELG",
        "outputId": "231e2e4a-d714-498b-af64-faf7be17e7e2"
      },
      "source": [
        "spacy.explain('PER')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Named person or family.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HCJcoEowNCY"
      },
      "source": [
        "Notem que o modelo não identificou a `data` na sentença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3AHxaEE9wWra",
        "outputId": "cfad5c1d-486a-4e40-8412-935629811593"
      },
      "source": [
        "texto = nlp('Data de Nascimento: 21/03/1960')\n",
        "\n",
        "displacy.render(texto, style='ent', jupyter=True);"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Data de \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Nascimento\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ": 21/03/1960</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yjP-3Yt3w1CZ",
        "outputId": "f2371d91-aacc-441d-e3c7-4735917e2851"
      },
      "source": [
        "texto = nlp('No dia 03/11/2020 a Joana foi aprovada no concurso')\n",
        "\n",
        "displacy.render(texto, style='ent', jupyter=True);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">No dia 03/11/2020 a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Joana\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " foi aprovada no concurso</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_rSIJS1xGon"
      },
      "source": [
        "Nos dois exemplos acima, o modelo também não identificou as entidades `data` nas sentenças."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2APIb_MxKc1"
      },
      "source": [
        "### **Como treinar NER a partir de um modelo spaCy vazio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foU1qxb-qGXH"
      },
      "source": [
        "Vamos analisar o seguinte exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6k-IiT5UIlau",
        "outputId": "4cd7a363-1109-496a-b2c2-3a3204377fd7"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(\"I ate Sushi yesterday. Maggi is a common fast food\")\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ate \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sushi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    yesterday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Maggi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is a common fast food</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w0KMbMdxNO8"
      },
      "source": [
        "Se você não quiser usar um modelo pré-existente, você pode criar um modelo vazio usando `spacy.blank()` apenas passando o ID do idioma. Para criar um modelo vazio no idioma inglês, você deve passar \"en\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nys79hhzpWr3"
      },
      "source": [
        "O código a seguir mostra as etapas iniciais para treinar o NER de um novo modelo vazio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywnRAIMvAqg-",
        "outputId": "5c602fa7-b05b-47fb-e09d-e493cf1789e8"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp=spacy.blank(\"en\")\n",
        "\n",
        "nlp.add_pipe(nlp.create_pipe('ner'))\n",
        "\n",
        "nlp.begin_training()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thinc.neural.optimizers.Optimizer at 0x7f6bfc22d510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxxDwtxSsVnQ"
      },
      "source": [
        "Depois disso, você pode seguir exatamente o mesmo procedimento como no caso do modelo pré-existente. É assim que você pode treinar o reconhecedor de entidade nomeada para identificar e categorizar corretamente de acordo com o contexto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zPnar6mso3n"
      },
      "source": [
        "E se você quiser colocar uma entidade em uma categoria que ainda não está presente?\n",
        "\n",
        "Considere que você tem muitos dados de texto sobre os alimentos consumidos em diversas áreas. E você deseja que o NER classifique todos os itens alimentares na categoria **`FOOD`**. Mas, não existe tal categoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9mUS0uxs1TE"
      },
      "source": [
        "`spaCy` é altamente flexível e permite adicionar um novo tipo de entidade e treinar o modelo. Esse recurso é extremamente útil, pois permite adicionar novos tipos de entidades para facilitar a recuperação de informações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhZLH9ins-bV"
      },
      "source": [
        "Primeiro, carregue o modelo de espaço pré-existente que deseja usar e obtenha o pipeline `ner` por meio do método `get_pipe()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW2Mn_PfDKah"
      },
      "source": [
        "# import and load the spacy model\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\") \n",
        "\n",
        "# Getting the ner component\n",
        "ner=nlp.get_pipe('ner')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWvtv2BHt1ug"
      },
      "source": [
        "Em seguida, armazene o nome do novo tipo de categoria/entidade em uma variável de string LABEL.\n",
        "\n",
        "Agora, como o modelo saberá quais entidades devem ser classificadas sob o novo rótulo?\n",
        "\n",
        "Você terá que treinar o modelo com exemplos. Os exemplos de treinamento devem ensinar ao modelo que tipo de entidades devem ser classificadas como `FOOD`.\n",
        "\n",
        "O formato dos dados de treinamento é uma lista de tuplas. Cada tupla contém o texto de exemplo e um dicionário. O dicionário terá as entidades chave, que armazenam os índices inicial e final junto com o rótulo das entidades presentes no texto.\n",
        "\n",
        ">\n",
        "Por exemplo, para passar \"Pizza is a common fast food\" como exemplo, o formato será: \n",
        " > `(\"Pizza is a common fast food\", {\"entities\": [(0, 5, \"FOOD\")]})`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9otI-PXKIL-J"
      },
      "source": [
        "# new label to add\n",
        "LABEL = \"FOOD\"\n",
        "\n",
        "# Training examples in the required format\n",
        "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, LABEL)]}),\n",
        "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, LABEL)]}),\n",
        "              (\"China's noodles are very famous\", {\"entities\": [(8,14, LABEL)]}),\n",
        "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, LABEL)]}),\n",
        "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, LABEL)]}),\n",
        "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, LABEL)]}),\n",
        "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, LABEL)]}),\n",
        "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, LABEL)]}),\n",
        "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, LABEL)]}),\n",
        "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, LABEL)]}),\n",
        "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, LABEL)]}),\n",
        "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, LABEL)]}),\n",
        "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, LABEL)]})\n",
        "           ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Z3uWT6umTE"
      },
      "source": [
        "Agora que os dados de treinamento estão prontos, podemos prosseguir para ver como esses exemplos são usados para treinar o NER.\n",
        "\n",
        "Lembre-se de que o rótulo “FOOD” não é conhecido pelo modelo agora.\n",
        "\n",
        "Portanto, nossa primeira tarefa será adicionar o rótulo ao NER através do método `add_label()`. Em seguida, você pode usar a função `resume_training()` para retornar um otimizador.\n",
        "\n",
        "Além disso, quando o treinamento é concluído, os outros componentes do pipeline também são afetados. Para evitar isso, use o método `disable_pipes()` para desabilitar todos os outros pipes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfuFk6cfHItb"
      },
      "source": [
        "# add the new label to ner\n",
        "ner.add_label(LABEL)\n",
        "\n",
        "# resume training\n",
        "optimizer = nlp.resume_training()\n",
        "move_names = list(ner.move_names)\n",
        "\n",
        "# list of pipes you want to train\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "\n",
        "# list of pipes which should remain unaffected in training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0L71Jz-vYjw"
      },
      "source": [
        "Algumas considerações:\n",
        "\n",
        "> a) Você tem que passar os exemplos pelo modelo pelo um número suficiente de iterações. Aqui, nós definimos 30 iterações.\n",
        "\n",
        "> b) Lembre-se de ajustar o modelo de iterações de acordo com o desempenho. Além disso, antes de cada iteração, é melhor embaralhar os exemplos aleatoriamente por meio da função `random.shuffle()`. Isso garantirá que o modelo não faça generalizações com base na ordem dos exemplos.\n",
        "\n",
        "> c) Os dados de treinamento devem ser passados em lotes. Você pode chamar a função `minibatch()` do `spaCy` sobre os exemplos de treinamento que retornarão seus dados em lotes. Um parâmetro da função de `minibatch` é `size`, denotando o tamanho do lote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ-qTwWIHSFM",
        "outputId": "7770bab8-5cd2-43f5-efcb-6e10a77c8fe2"
      },
      "source": [
        "# importing requirements\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "# Begin training by disabling other pipeline components\n",
        "with nlp.disable_pipes(*other_pipes) :\n",
        "\n",
        "  sizes = compounding(1.0, 4.0, 1.001)\n",
        "  # training for 30 iterations     \n",
        "  for itn in range(30):\n",
        "    # shuffle examples before training\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "    # dictionary to store losses\n",
        "    losses = {}\n",
        "    for batch in batches:\n",
        "      texts, annotations = zip(*batch)\n",
        "      # Calling update() over the iteration\n",
        "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "      print(\"Losses\", losses)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses {'ner': 2.3995642220727347}\n",
            "Losses {'ner': 10.304034546589032}\n",
            "Losses {'ner': 13.305751696353019}\n",
            "Losses {'ner': 15.496271063057412}\n",
            "Losses {'ner': 19.265382820413173}\n",
            "Losses {'ner': 25.35750985123493}\n",
            "Losses {'ner': 32.311582422113226}\n",
            "Losses {'ner': 37.68660711853605}\n",
            "Losses {'ner': 45.81508636086088}\n",
            "Losses {'ner': 51.72462280334745}\n",
            "Losses {'ner': 59.403900008438235}\n",
            "Losses {'ner': 63.18525014978595}\n",
            "Losses {'ner': 67.31209391328903}\n",
            "Losses {'ner': 4.856054702590484}\n",
            "Losses {'ner': 9.289830486527933}\n",
            "Losses {'ner': 15.575885139613035}\n",
            "Losses {'ner': 18.49907881371533}\n",
            "Losses {'ner': 24.884713728208823}\n",
            "Losses {'ner': 31.58632288419963}\n",
            "Losses {'ner': 35.11501042136834}\n",
            "Losses {'ner': 40.84024588355706}\n",
            "Losses {'ner': 46.706811799931444}\n",
            "Losses {'ner': 57.1679939173936}\n",
            "Losses {'ner': 62.54544261876609}\n",
            "Losses {'ner': 64.48343722222297}\n",
            "Losses {'ner': 71.91696004269569}\n",
            "Losses {'ner': 2.9678623771214916}\n",
            "Losses {'ner': 3.8934194141911576}\n",
            "Losses {'ner': 5.1079882336607625}\n",
            "Losses {'ner': 8.07155620472713}\n",
            "Losses {'ner': 12.65020950808048}\n",
            "Losses {'ner': 16.20615061087392}\n",
            "Losses {'ner': 21.893556845099624}\n",
            "Losses {'ner': 24.14638824128076}\n",
            "Losses {'ner': 30.730978298599325}\n",
            "Losses {'ner': 33.072237745151256}\n",
            "Losses {'ner': 34.78503490373521}\n",
            "Losses {'ner': 38.05989938293612}\n",
            "Losses {'ner': 41.991155124068314}\n",
            "Losses {'ner': 4.1717075731939985}\n",
            "Losses {'ner': 4.896690292851417}\n",
            "Losses {'ner': 6.794431935399189}\n",
            "Losses {'ner': 10.349344349393505}\n",
            "Losses {'ner': 13.0245668724383}\n",
            "Losses {'ner': 17.205245390694472}\n",
            "Losses {'ner': 22.45678384245548}\n",
            "Losses {'ner': 25.497047770841164}\n",
            "Losses {'ner': 35.341584937719745}\n",
            "Losses {'ner': 37.028557936122525}\n",
            "Losses {'ner': 41.326840588691994}\n",
            "Losses {'ner': 43.77609707856027}\n",
            "Losses {'ner': 46.64295682961529}\n",
            "Losses {'ner': 4.4775789473205805}\n",
            "Losses {'ner': 9.995814586756751}\n",
            "Losses {'ner': 12.616506724443752}\n",
            "Losses {'ner': 16.704067861661315}\n",
            "Losses {'ner': 19.287556475028396}\n",
            "Losses {'ner': 22.29000681079924}\n",
            "Losses {'ner': 26.285963692702353}\n",
            "Losses {'ner': 27.843406182189483}\n",
            "Losses {'ner': 32.86777593864781}\n",
            "Losses {'ner': 35.10986731924095}\n",
            "Losses {'ner': 41.272315723597785}\n",
            "Losses {'ner': 46.683953977897545}\n",
            "Losses {'ner': 52.11972507096107}\n",
            "Losses {'ner': 2.5564178030292624}\n",
            "Losses {'ner': 3.9959304021142543}\n",
            "Losses {'ner': 8.916261186397833}\n",
            "Losses {'ner': 13.283656188792747}\n",
            "Losses {'ner': 18.414947008288664}\n",
            "Losses {'ner': 21.082462680333492}\n",
            "Losses {'ner': 24.32857999491921}\n",
            "Losses {'ner': 29.913358795525937}\n",
            "Losses {'ner': 32.5270146276564}\n",
            "Losses {'ner': 35.046675022295176}\n",
            "Losses {'ner': 41.47375255410975}\n",
            "Losses {'ner': 47.10714394466436}\n",
            "Losses {'ner': 51.212480491562246}\n",
            "Losses {'ner': 1.3121103346347809}\n",
            "Losses {'ner': 7.6520387440687045}\n",
            "Losses {'ner': 16.63990355131682}\n",
            "Losses {'ner': 18.17322056356352}\n",
            "Losses {'ner': 22.295247958623804}\n",
            "Losses {'ner': 26.60223781655077}\n",
            "Losses {'ner': 31.867184849907062}\n",
            "Losses {'ner': 37.20687120103685}\n",
            "Losses {'ner': 39.0556967563316}\n",
            "Losses {'ner': 40.324619787497795}\n",
            "Losses {'ner': 44.78765702176315}\n",
            "Losses {'ner': 47.595324483918375}\n",
            "Losses {'ner': 47.81920156649721}\n",
            "Losses {'ner': 2.78322016261518}\n",
            "Losses {'ner': 10.337867209687829}\n",
            "Losses {'ner': 15.03071535564959}\n",
            "Losses {'ner': 17.931505660060793}\n",
            "Losses {'ner': 21.39644585410133}\n",
            "Losses {'ner': 23.901855060248636}\n",
            "Losses {'ner': 27.719974239706062}\n",
            "Losses {'ner': 34.67501791322138}\n",
            "Losses {'ner': 44.804183443426155}\n",
            "Losses {'ner': 46.070090150868054}\n",
            "Losses {'ner': 49.050577205081936}\n",
            "Losses {'ner': 51.191034222225426}\n",
            "Losses {'ner': 55.13379026504117}\n",
            "Losses {'ner': 3.3529044445604086}\n",
            "Losses {'ner': 8.268728638067842}\n",
            "Losses {'ner': 10.865547215595143}\n",
            "Losses {'ner': 15.011672120570438}\n",
            "Losses {'ner': 17.663112576614367}\n",
            "Losses {'ner': 24.749202698032605}\n",
            "Losses {'ner': 27.53640117359464}\n",
            "Losses {'ner': 27.576192182285013}\n",
            "Losses {'ner': 33.6203315675084}\n",
            "Losses {'ner': 40.364015283383196}\n",
            "Losses {'ner': 48.174816520680906}\n",
            "Losses {'ner': 50.5815968613897}\n",
            "Losses {'ner': 54.62096615464543}\n",
            "Losses {'ner': 6.935673062631395}\n",
            "Losses {'ner': 10.277287490607705}\n",
            "Losses {'ner': 14.10991187975742}\n",
            "Losses {'ner': 18.359326041070744}\n",
            "Losses {'ner': 23.27044605719857}\n",
            "Losses {'ner': 26.972536403278355}\n",
            "Losses {'ner': 30.99460329006729}\n",
            "Losses {'ner': 34.46311143985554}\n",
            "Losses {'ner': 38.75746034809458}\n",
            "Losses {'ner': 46.451246134723306}\n",
            "Losses {'ner': 50.42386987364921}\n",
            "Losses {'ner': 54.97044531627512}\n",
            "Losses {'ner': 60.685540649418954}\n",
            "Losses {'ner': 3.229449762031436}\n",
            "Losses {'ner': 9.397968424484134}\n",
            "Losses {'ner': 11.492903494625352}\n",
            "Losses {'ner': 13.57674417772796}\n",
            "Losses {'ner': 19.22079140821006}\n",
            "Losses {'ner': 20.29920116707217}\n",
            "Losses {'ner': 22.424006317010935}\n",
            "Losses {'ner': 29.324035307592567}\n",
            "Losses {'ner': 32.5153998045389}\n",
            "Losses {'ner': 34.27148882040274}\n",
            "Losses {'ner': 37.004045692938234}\n",
            "Losses {'ner': 37.03788288784199}\n",
            "Losses {'ner': 41.56505988312347}\n",
            "Losses {'ner': 4.967269960092381}\n",
            "Losses {'ner': 10.334866270190105}\n",
            "Losses {'ner': 15.743908308934806}\n",
            "Losses {'ner': 19.74175937635391}\n",
            "Losses {'ner': 21.331020972445003}\n",
            "Losses {'ner': 23.800138893247095}\n",
            "Losses {'ner': 26.074453641370383}\n",
            "Losses {'ner': 28.878851046728414}\n",
            "Losses {'ner': 33.55122738913724}\n",
            "Losses {'ner': 35.606187363083336}\n",
            "Losses {'ner': 42.71718288505599}\n",
            "Losses {'ner': 42.7324857593494}\n",
            "Losses {'ner': 44.0452955102819}\n",
            "Losses {'ner': 2.739792471867986}\n",
            "Losses {'ner': 3.7519610955660028}\n",
            "Losses {'ner': 8.72081984237775}\n",
            "Losses {'ner': 10.405865584109051}\n",
            "Losses {'ner': 12.985427051928127}\n",
            "Losses {'ner': 18.44912189259776}\n",
            "Losses {'ner': 26.26896508797654}\n",
            "Losses {'ner': 26.437001844780752}\n",
            "Losses {'ner': 31.83693257962142}\n",
            "Losses {'ner': 35.235907222441085}\n",
            "Losses {'ner': 37.530898431147136}\n",
            "Losses {'ner': 43.027691380883255}\n",
            "Losses {'ner': 48.0192577222536}\n",
            "Losses {'ner': 4.331047654246504}\n",
            "Losses {'ner': 8.80143613015025}\n",
            "Losses {'ner': 11.797510797932773}\n",
            "Losses {'ner': 18.680440681830078}\n",
            "Losses {'ner': 21.92596049090571}\n",
            "Losses {'ner': 28.1591311492557}\n",
            "Losses {'ner': 33.70275005085932}\n",
            "Losses {'ner': 40.10570254309641}\n",
            "Losses {'ner': 44.03559597900676}\n",
            "Losses {'ner': 49.5377794066917}\n",
            "Losses {'ner': 56.25533558526149}\n",
            "Losses {'ner': 59.14012858090064}\n",
            "Losses {'ner': 59.4662481076557}\n",
            "Losses {'ner': 5.316894085728563}\n",
            "Losses {'ner': 9.866563855321147}\n",
            "Losses {'ner': 12.135076927375849}\n",
            "Losses {'ner': 18.312161876243408}\n",
            "Losses {'ner': 18.47032665124243}\n",
            "Losses {'ner': 23.15168968082253}\n",
            "Losses {'ner': 27.047184000321067}\n",
            "Losses {'ner': 32.02425302059237}\n",
            "Losses {'ner': 37.683819635500186}\n",
            "Losses {'ner': 39.65376812355498}\n",
            "Losses {'ner': 47.751110046594476}\n",
            "Losses {'ner': 54.60606387206826}\n",
            "Losses {'ner': 61.31326627296403}\n",
            "Losses {'ner': 3.5686916513368487}\n",
            "Losses {'ner': 8.980129355797544}\n",
            "Losses {'ner': 13.769125128397718}\n",
            "Losses {'ner': 14.110719208838418}\n",
            "Losses {'ner': 17.76572084557847}\n",
            "Losses {'ner': 21.41574114692048}\n",
            "Losses {'ner': 24.930615068238694}\n",
            "Losses {'ner': 30.221775094105396}\n",
            "Losses {'ner': 33.4816210619756}\n",
            "Losses {'ner': 38.8949328349554}\n",
            "Losses {'ner': 44.3144288645708}\n",
            "Losses {'ner': 48.1305843317532}\n",
            "Losses {'ner': 51.528903207334224}\n",
            "Losses {'ner': 4.973157465457916}\n",
            "Losses {'ner': 7.9856220382935135}\n",
            "Losses {'ner': 11.532577515676167}\n",
            "Losses {'ner': 11.556808058068782}\n",
            "Losses {'ner': 14.64428515378495}\n",
            "Losses {'ner': 19.140345070291005}\n",
            "Losses {'ner': 26.43835597090765}\n",
            "Losses {'ner': 32.21528798854888}\n",
            "Losses {'ner': 36.13405210359815}\n",
            "Losses {'ner': 40.732256283177776}\n",
            "Losses {'ner': 45.9460355646097}\n",
            "Losses {'ner': 50.59857742368979}\n",
            "Losses {'ner': 56.11804628625316}\n",
            "Losses {'ner': 1.028043401389823}\n",
            "Losses {'ner': 2.0447045879468533}\n",
            "Losses {'ner': 5.565547149445138}\n",
            "Losses {'ner': 8.98970711422453}\n",
            "Losses {'ner': 13.067596078259612}\n",
            "Losses {'ner': 15.370745396232905}\n",
            "Losses {'ner': 17.986521144861854}\n",
            "Losses {'ner': 20.358795982245738}\n",
            "Losses {'ner': 22.22170539706417}\n",
            "Losses {'ner': 27.11597111023434}\n",
            "Losses {'ner': 28.24262666230493}\n",
            "Losses {'ner': 29.33752337173496}\n",
            "Losses {'ner': 31.34504470630486}\n",
            "Losses {'ner': 0.0005606649447145173}\n",
            "Losses {'ner': 4.725663438695847}\n",
            "Losses {'ner': 8.21941930840876}\n",
            "Losses {'ner': 11.135916788833129}\n",
            "Losses {'ner': 16.201765709327447}\n",
            "Losses {'ner': 18.733377383792686}\n",
            "Losses {'ner': 19.705023811980254}\n",
            "Losses {'ner': 22.42382487780995}\n",
            "Losses {'ner': 29.125341845942586}\n",
            "Losses {'ner': 31.11093446279918}\n",
            "Losses {'ner': 35.4086416637663}\n",
            "Losses {'ner': 39.74094737415251}\n",
            "Losses {'ner': 45.23774318491158}\n",
            "Losses {'ner': 3.355893942672992}\n",
            "Losses {'ner': 9.174203320799279}\n",
            "Losses {'ner': 14.52418093329311}\n",
            "Losses {'ner': 16.75747213759911}\n",
            "Losses {'ner': 21.90110229642937}\n",
            "Losses {'ner': 27.801960450804245}\n",
            "Losses {'ner': 35.32167495934459}\n",
            "Losses {'ner': 38.22210107600017}\n",
            "Losses {'ner': 40.16558256443}\n",
            "Losses {'ner': 44.92609039394432}\n",
            "Losses {'ner': 49.5394210057288}\n",
            "Losses {'ner': 53.828298564052545}\n",
            "Losses {'ner': 60.26545860151049}\n",
            "Losses {'ner': 2.1450066640536534}\n",
            "Losses {'ner': 5.316365135899105}\n",
            "Losses {'ner': 5.447333221214649}\n",
            "Losses {'ner': 8.316758402716914}\n",
            "Losses {'ner': 10.837876091230157}\n",
            "Losses {'ner': 15.69106784694219}\n",
            "Losses {'ner': 17.522521127505854}\n",
            "Losses {'ner': 17.530511052776376}\n",
            "Losses {'ner': 20.968242795635263}\n",
            "Losses {'ner': 24.787152459420042}\n",
            "Losses {'ner': 28.877318504525476}\n",
            "Losses {'ner': 33.05445542201181}\n",
            "Losses {'ner': 37.71679704773254}\n",
            "Losses {'ner': 2.5990035448921844}\n",
            "Losses {'ner': 11.116008690441959}\n",
            "Losses {'ner': 18.831099515977257}\n",
            "Losses {'ner': 18.869947338450856}\n",
            "Losses {'ner': 22.683627044406876}\n",
            "Losses {'ner': 28.36033143810664}\n",
            "Losses {'ner': 34.85437623304051}\n",
            "Losses {'ner': 36.99405691130096}\n",
            "Losses {'ner': 40.08487296768979}\n",
            "Losses {'ner': 40.086135628130506}\n",
            "Losses {'ner': 42.79879475008718}\n",
            "Losses {'ner': 44.35145439419219}\n",
            "Losses {'ner': 45.4414474837279}\n",
            "Losses {'ner': 5.223681989591569}\n",
            "Losses {'ner': 5.245684026333038}\n",
            "Losses {'ner': 5.247040717999084}\n",
            "Losses {'ner': 10.632566321883246}\n",
            "Losses {'ner': 14.579532009482534}\n",
            "Losses {'ner': 18.85720531481678}\n",
            "Losses {'ner': 26.655184133233377}\n",
            "Losses {'ner': 29.130814956140437}\n",
            "Losses {'ner': 33.14288784087255}\n",
            "Losses {'ner': 35.9231568677812}\n",
            "Losses {'ner': 38.00344241227659}\n",
            "Losses {'ner': 41.420358418404895}\n",
            "Losses {'ner': 45.21301336000462}\n",
            "Losses {'ner': 6.215083130635321}\n",
            "Losses {'ner': 8.938257806468755}\n",
            "Losses {'ner': 12.01020849854467}\n",
            "Losses {'ner': 16.496548149592854}\n",
            "Losses {'ner': 23.71255764723537}\n",
            "Losses {'ner': 31.018692169196584}\n",
            "Losses {'ner': 37.47467755791922}\n",
            "Losses {'ner': 40.90603369134669}\n",
            "Losses {'ner': 44.01230696399685}\n",
            "Losses {'ner': 51.519996169926486}\n",
            "Losses {'ner': 54.959494105349116}\n",
            "Losses {'ner': 57.51765987275122}\n",
            "Losses {'ner': 60.62559879759435}\n",
            "Losses {'ner': 3.534000316421327}\n",
            "Losses {'ner': 8.29128204584896}\n",
            "Losses {'ner': 11.556401725102035}\n",
            "Losses {'ner': 18.277499107658798}\n",
            "Losses {'ner': 22.01176858193054}\n",
            "Losses {'ner': 24.548819294189432}\n",
            "Losses {'ner': 28.797459473823526}\n",
            "Losses {'ner': 30.941770306574742}\n",
            "Losses {'ner': 34.702470475244894}\n",
            "Losses {'ner': 43.03399450508468}\n",
            "Losses {'ner': 46.349550733166865}\n",
            "Losses {'ner': 53.11308340151899}\n",
            "Losses {'ner': 58.22490940085717}\n",
            "Losses {'ner': 6.350490096025169}\n",
            "Losses {'ner': 7.316116818308586}\n",
            "Losses {'ner': 7.395739338320624}\n",
            "Losses {'ner': 7.398080874404229}\n",
            "Losses {'ner': 13.259956615215458}\n",
            "Losses {'ner': 15.204142644893125}\n",
            "Losses {'ner': 18.05064951248727}\n",
            "Losses {'ner': 20.627853045382096}\n",
            "Losses {'ner': 22.89688324488469}\n",
            "Losses {'ner': 26.189922397283397}\n",
            "Losses {'ner': 31.75996145771586}\n",
            "Losses {'ner': 36.00149456205344}\n",
            "Losses {'ner': 42.111327261929716}\n",
            "Losses {'ner': 0.001253565669437684}\n",
            "Losses {'ner': 3.4288209098157063}\n",
            "Losses {'ner': 6.790787989157118}\n",
            "Losses {'ner': 11.48719988286016}\n",
            "Losses {'ner': 15.750954359690105}\n",
            "Losses {'ner': 15.765955404255102}\n",
            "Losses {'ner': 21.54977746521624}\n",
            "Losses {'ner': 24.42894256558792}\n",
            "Losses {'ner': 29.02194666817103}\n",
            "Losses {'ner': 33.955284043692686}\n",
            "Losses {'ner': 39.439924623916326}\n",
            "Losses {'ner': 41.18325692592698}\n",
            "Losses {'ner': 46.88761521168563}\n",
            "Losses {'ner': 1.8904311104342923}\n",
            "Losses {'ner': 3.8845313932349654}\n",
            "Losses {'ner': 7.220546486084231}\n",
            "Losses {'ner': 14.058185631637343}\n",
            "Losses {'ner': 17.32128827180236}\n",
            "Losses {'ner': 19.38607685276729}\n",
            "Losses {'ner': 23.0000953676943}\n",
            "Losses {'ner': 26.233902687469524}\n",
            "Losses {'ner': 29.111529522838097}\n",
            "Losses {'ner': 34.399661568750886}\n",
            "Losses {'ner': 40.17790426076424}\n",
            "Losses {'ner': 43.27973455281635}\n",
            "Losses {'ner': 46.290438293063104}\n",
            "Losses {'ner': 6.330673430318711}\n",
            "Losses {'ner': 10.362086937619551}\n",
            "Losses {'ner': 12.04773777413402}\n",
            "Losses {'ner': 19.532864381254058}\n",
            "Losses {'ner': 26.293601567668247}\n",
            "Losses {'ner': 29.246156601344865}\n",
            "Losses {'ner': 35.41483380283944}\n",
            "Losses {'ner': 38.21067246223083}\n",
            "Losses {'ner': 45.326742371416}\n",
            "Losses {'ner': 48.903415074884066}\n",
            "Losses {'ner': 52.91963401281751}\n",
            "Losses {'ner': 53.89830027706117}\n",
            "Losses {'ner': 58.545585170816736}\n",
            "Losses {'ner': 5.121552566735772}\n",
            "Losses {'ner': 9.643546244095567}\n",
            "Losses {'ner': 11.917469518607184}\n",
            "Losses {'ner': 12.895779117345818}\n",
            "Losses {'ner': 19.167960424202327}\n",
            "Losses {'ner': 21.835398890336045}\n",
            "Losses {'ner': 22.889694343702693}\n",
            "Losses {'ner': 25.526276388951054}\n",
            "Losses {'ner': 29.22747120279714}\n",
            "Losses {'ner': 33.00861455685267}\n",
            "Losses {'ner': 34.87190933046399}\n",
            "Losses {'ner': 37.51440400337337}\n",
            "Losses {'ner': 43.24127528523563}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN7TNedZwN_4"
      },
      "source": [
        "Para cada iteração, o modelo é atualizado por meio do comando `nlp.update()`. Os parâmetros de `nlp.update()` são:\n",
        "\n",
        "* `docs`: espera um lote de textos como entrada. Você pode passar cada lote para o método `zip`, que retornará lotes de texto e anotações.\n",
        "     `\n",
        "* `sgd`: você deve passar o otimizador que foi retornado por `resume_training()` aqui.\n",
        "\n",
        "* `golds`: você pode passar as anotações que obtivemos por meio do método `zip` aqui.\n",
        "\n",
        "* `drop`: representa a taxa de abandono.\n",
        "\n",
        "* `losses`: um dicionário para conter as perdas em cada componente do pipeline. Crie um dicionário vazio e passe-o aqui.\n",
        "\n",
        "A cada palavra, `update()` faz uma previsão. Em seguida, ele consulta as anotações para verificar se a previsão está correta. Se não estiver, ele ajusta os pesos para que a ação correta tenha uma pontuação mais alta na próxima vez."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0j85t7c3Y2A"
      },
      "source": [
        "O treinamento de nosso NER está concluído agora. Vamos testar se o NER pode identificar nossa nova entidade. Se não corresponder às suas expectativas, tente incluir mais exemplos de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1gxntb0tq3R-",
        "outputId": "52f066ef-d002-427d-88c9-100cc43b32ae"
      },
      "source": [
        "doc = nlp(\"I ate Sushi yesterday. Maggi is a common fast food\")\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ate \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sushi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
              "</mark>\n",
              " yesterday. \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Maggi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
              "</mark>\n",
              " is a common fast food</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q82xaYZxo3s"
      },
      "source": [
        "Observe a saída acima. O modelo identificou corretamente os itens `FOOD`. Além disso, observe que não passamos \"Maggi\" como um exemplo de treinamento para a modelo. Ainda assim, com base na similaridade de contexto, o modelo identificou \"Maggi\" também como `FOOD`. \n",
        "\n",
        ">\n",
        "**Este é um requisito importante! Nosso modelo não deve apenas memorizar os exemplos de treinamento. Deve aprender com eles e generalizar para novos exemplos.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Ajbaz0yBHx"
      },
      "source": [
        "Depois de encontrar o desempenho do modelo satisfatório, você pode salvar o modelo atualizado no diretório usando o comando `to_disk`. Você também pode carregar o modelo do diretório a qualquer momento, passando o caminho do diretório para a função `spacy.load()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMplc-mdyKIf"
      },
      "source": [
        "# output directory\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir=Path('/content/modelo')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXbAppweyQHx",
        "outputId": "5ce3329d-b533-45dc-ca2c-c59f4098e8c1"
      },
      "source": [
        "# saving the model to the output directory\n",
        "if not output_dir.exists():\n",
        "  output_dir.mkdir()\n",
        "nlp.meta['name'] = 'my_ner'  # rename model\n",
        "nlp.to_disk(output_dir)\n",
        "print(\"Saved model to\", output_dir)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to /content/modelo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQKaF09jyTSX",
        "outputId": "06bdc5a9-a2cf-49c1-fb8c-8ece4fb1addf"
      },
      "source": [
        "# loading the model from the directory\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "\n",
        "#assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
        "doc2 = nlp2('Dosa is an extremely famous south Indian dish')\n",
        "\n",
        "for ent in doc2.ents:\n",
        "  print(ent.label_, ent.text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from /content/modelo\n",
            "FOOD Dosa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "OMOcsGbtycUA",
        "outputId": "7602fd9f-d146-421d-a750-3ebec3990ace"
      },
      "source": [
        "displacy.render(doc2, style='ent', jupyter=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dosa\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
              "</mark>\n",
              " is an extremely famous south Indian dish</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1BO1LsYEW67"
      },
      "source": [
        "**Referência**:\n",
        "\n",
        "*How to Train spaCy to Autodetect New Entities (NER) [Complete Guide]*\n",
        "> https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/"
      ]
    }
  ]
}