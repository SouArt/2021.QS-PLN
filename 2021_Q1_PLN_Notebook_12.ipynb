{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-Q1 PLN Notebook 12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUZmgXcktxqHMVAZBF9/fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2021.QS-PLN/blob/main/2021_Q1_PLN_Notebook_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f91gjdw4PmTL"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2021.Q1]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLYpRqBfPt8_"
      },
      "source": [
        "# **Classificação de Texto com spaCy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTeM3hDqP8nn"
      },
      "source": [
        "Uma tarefa comum em PLN é a **classificação de texto**. Isso é \"classificação\" no sentido convencional de aprendizado de máquina, porém é aplicada a texto. Os exemplos incluem detecção de *spam*, análise de sentimento etc.\n",
        "\n",
        "Neste exemplo, veremos como fazer a classificação de texto usando a biblioteca **`spaCy`**. O **`spaCy`** é uma biblioteca de processamento de linguagem natural (para Python) que tem desde funcionalidades \"básicas\" como tokenização, que consiste em um pré-processamento do texto, até mais complexas que permitem treinar modelos estatísticos para classificação de textos.\n",
        "\n",
        "Neste exemplo, o classificador detectará mensagens de spam, uma funcionalidade comum na maioria dos clientes de e-mail. Aqui está uma visão geral dos dados que usaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0erfNSdGRc4Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7b41c795-689e-4e8b-f748-8a75b66ce59b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# dataset: https://www.kaggle.com/matleonard/nlp-course\n",
        "\n",
        "# loading the spam data\n",
        "# ham is the label for non-spam messages\n",
        "spam = pd.read_csv('/content/spam_kaggle.csv', encoding='latin-1')\n",
        "spam.head(10)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>Even my brother is not like to speak with me. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ham</td>\n",
              "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>WINNER!! As a valued network customer you have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
              "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
              "6   ham  Even my brother is not like to speak with me. ...\n",
              "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
              "8  spam  WINNER!! As a valued network customer you have...\n",
              "9  spam  Had your mobile 11 months or more? U R entitle..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr88--QXTlBl"
      },
      "source": [
        "***Bag of Words***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmYRI2DaToMG"
      },
      "source": [
        "Os modelos de aprendizado de máquina não aprendem com dados de texto brutos. Em vez disso, você precisa converter o texto em algo numérico.\n",
        "\n",
        "A representação mais simples é uma variação da codificação *One-Hot*. Você representa cada documento como um vetor de frequências de termos para cada termo no vocabulário. O vocabulário é construído a partir de todos os tokens (termos) no corpus (a coleção de documentos).\n",
        "\n",
        "Como exemplo, tome as frases \"Chá é vida. Chá é amor.\" e \"Chá é saudável, calmante e delicioso.\" como nosso corpus. O vocabulário então é {\"chá\", \"é\", \"vida\", \"amor\", \"saudável\", \"calmante\", \"e\", \"delicioso\"} (ignorando a pontuação).\n",
        "\n",
        "Para cada documento, conte quantas vezes um termo ocorre e coloque essa contagem no elemento apropriado de um vetor. A primeira frase tem \"chá\" duas vezes e essa é a primeira posição em nosso vocabulário, então colocamos o número 2 no primeiro elemento do vetor. Nossas frases como vetores se parecem com:\n",
        "\n",
        "> v1=[2  2  1  1  0  0  0  0]\n",
        "\n",
        "> v2=[1  1  0  0  1  1  1  1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCB4WpErUkb3"
      },
      "source": [
        "Isso é chamado de representação do ***bag of words*** (sacola de palavras). Você pode ver que documentos com termos semelhantes terão vetores semelhantes. Os vocabulários freqüentemente têm dezenas de milhares de termos, então esses vetores podem ser muito grandes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS_ZR4JaU9MY"
      },
      "source": [
        "### **Construindo um modelo *Bag of Words***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLueKjNvVCCJ"
      },
      "source": [
        "Depois de ter seus documentos em uma representação *Bag of Words*, você pode usar esses vetores como entrada para qualquer modelo de aprendizado de máquina. **`spaCy`** lida com a conversão de palavras e construindo um modelo linear simples para você com a classe `TextCategorizer`.\n",
        "\n",
        "O *`TextCategorizer`* é um pipe spaCy. Pipes são classes para processamento e transformação de tokens. Quando você cria um modelo spaCy com `nlp = spacy.load ('en_core_web_sm')`, há pipes default que realizam marcação de parte da fala (*part of speech tagging*), reconhecimento de entidade e outras transformações. \n",
        "\n",
        ">\n",
        "Quando você executa o texto por meio de um modelo `doc = nlp (\"Algum texto aqui\")`, a saída dos pipes são anexados aos tokens no objeto `doc`. Os lemas para `token.lemma_` vêm de um desses pipes.\n",
        "\n",
        "Você pode remover ou adicionar pipes aos modelos. O que faremos aqui é criar um modelo vazio sem pipes (exceto um tokenizador, uma vez que todos os modelos sempre têm um tokenizador). Em seguida, criaremos um pipe `TextCategorizer` e o adicionaremos ao modelo vazio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d96FlrhmWQBn"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# create an empty model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
        "textcat = nlp.create_pipe(\"textcat\",\n",
        "                          config={\"exclusive_classes\": True, \"architecture\": \"bow\"})\n",
        "\n",
        "# the TextCategorizer to the empty model\n",
        "nlp.add_pipe(textcat)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWPFitBvWoID"
      },
      "source": [
        "Como as classes são de *ham* ou *spam*, definimos \"exclusive_classes\" como `True`. Também configuramos com a arquitetura *Bag of Words*\" (\"bow\"). \n",
        "\n",
        "Em seguida, adicionaremos os rótulos ao modelo. Aqui, \"*ham*\" são para mensagens reais, \"*spam*\" são mensagens de spam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY-EYxdOXExM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc667f6f-87cb-4e22-d044-fc7ae91d5271"
      },
      "source": [
        "# add labels to text classifier\n",
        "textcat.add_label(\"ham\")\n",
        "textcat.add_label(\"spam\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxnSTY0pXdMX"
      },
      "source": [
        "### **Treinando um modelo categorizador de texto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTb-72FyXjvT"
      },
      "source": [
        "Em seguida, você converterá os rótulos nos dados para o formulário que o `TextCategorizer` requer. Para cada documento, você criará um dicionário de valores booleanos para cada classe.\n",
        "\n",
        "Por exemplo, se um texto for \"ham\", precisamos de um dicionário {'ham': True, 'spam': False}. O modelo está procurando por esses rótulos em outro dicionário com a chave '*cats*' (categorias)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpGTtD6cX4FR"
      },
      "source": [
        "train_texts = spam['text'].values\n",
        "train_labels = [{'cats': {'ham': label == 'ham',\n",
        "                          'spam': label == 'spam'}} \n",
        "                for label in spam['label']]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zXM33WoZrRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c301f4ee-b031-4845-b53c-aa3e24fcb0d7"
      },
      "source": [
        "# then we combine the texts and labels into a single list\n",
        "train_data = list(zip(train_texts, train_labels))\n",
        "train_data[:3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
              "  {'cats': {'ham': True, 'spam': False}}),\n",
              " ('Ok lar... Joking wif u oni...', {'cats': {'ham': True, 'spam': False}}),\n",
              " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
              "  {'cats': {'ham': False, 'spam': True}})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTTOuFHOZ3Qp"
      },
      "source": [
        "Agora você está pronto para treinar o modelo. Primeiro, crie um otimizador usando `nlp.begin_training()`. **`spaCy`** usa esse otimizador para atualizar o modelo. Em geral, é mais eficiente treinar modelos em pequenos lotes. **`spaCy`** fornece a função de `minibatch` que retorna um gerador de *minibatches* para treinamento. Finalmente, os *minibatches* são divididos em textos e rótulos, e então usados com `nlp.update` para atualizar os parâmetros do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TBHwsDraV44"
      },
      "source": [
        "from spacy.util import minibatch\n",
        "\n",
        "spacy.util.fix_random_seed(1)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# create the batch generator with batch size = 8\n",
        "batches = minibatch(train_data, size=8)\n",
        "# Iterate through minibatches\n",
        "for batch in batches:\n",
        "    # Each batch is a list of (text, label) but we need to\n",
        "    # send separate lists for texts and labels to update().\n",
        "    # This is a quick way to split a list of tuples into lists\n",
        "    texts, labels = zip(*batch)\n",
        "    nlp.update(texts, labels, sgd=optimizer)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT-N171Ian1B"
      },
      "source": [
        "Este é apenas um loop de treinamento (ou época) através dos dados. O modelo normalmente precisará de várias épocas. Use outro loop para mais épocas e, opcionalmente, embaralhe novamente os dados de treinamento no início de cada loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6No-8XG4avfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9aa6473-9ace-4e5a-f032-23fb4ca99cfc"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "spacy.util.fix_random_seed(1)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "losses = {}\n",
        "for epoca in range(10):\n",
        "    random.shuffle(train_data)\n",
        "    # Create the batch generator with batch size = 8\n",
        "    batches = minibatch(train_data, size=8)\n",
        "    # Iterate through minibatches\n",
        "    for batch in batches:\n",
        "        # Each batch is a list of (text, label) but we need to\n",
        "        # send separate lists for texts and labels to update().\n",
        "        # This is a quick way to split a list of tuples into lists\n",
        "        texts, labels = zip(*batch)\n",
        "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
        "    print(losses)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'textcat': 0.4309954632938684}\n",
            "{'textcat': 0.6465297238214021}\n",
            "{'textcat': 0.7822909041447552}\n",
            "{'textcat': 0.8687101021063368}\n",
            "{'textcat': 0.9246688244620618}\n",
            "{'textcat': 0.9618610965655705}\n",
            "{'textcat': 0.9899194243909888}\n",
            "{'textcat': 1.008498932771929}\n",
            "{'textcat': 1.0229856369099788}\n",
            "{'textcat': 1.0330240397877635}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZ8g4Cpa4CB"
      },
      "source": [
        "### **Fazendo predições**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0047E0ha7Q3"
      },
      "source": [
        "Agora que você tem um modelo treinado, pode fazer predições com o método `predict()`. O texto de entrada precisa ser tokenizado com `nlp.tokenizer`. Em seguida, você passa os tokens para o método de previsão que retorna as ponderações (*score*). As ponderações são a probabilidade do texto de entrada pertencer às classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvYrtzvJb0-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb23025-e9c9-4ddd-f6f4-b5157ed8c1c9"
      },
      "source": [
        "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
        "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
        "docs = [nlp.tokenizer(text) for text in texts]\n",
        "    \n",
        "# use textcat to get the scores for each doc\n",
        "textcat = nlp.get_pipe('textcat')\n",
        "scores, _ = textcat.predict(docs)\n",
        "\n",
        "print(scores)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.9994600e-01 5.4017470e-05]\n",
            " [1.2052479e-02 9.8794752e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1wRVjKcCiU"
      },
      "source": [
        "As ponderações são usadas para prever uma única classe ou rótulo, escolhendo o rótulo com a maior probabilidade. Você obtém o índice de maior probabilidade com `scores.argmax` e, em seguida, usa o índice para obter o rótulo da string com `textcat.labels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSy-6LJlcX_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3db00da-c168-4d49-fe5b-c6351ab86dbb"
      },
      "source": [
        "# from the scores, find the label with the highest score/probability\n",
        "predicted_labels = scores.argmax(axis=1)\n",
        "print([textcat.labels[label] for label in predicted_labels])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham', 'spam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1z0wRAtcqv8"
      },
      "source": [
        "Avaliar o modelo é simples, uma vez que você tem as predições. Para medir a acurácia, calcule quantas predições corretas são feitas em alguns dados de teste, dividido pelo número total de predições."
      ]
    }
  ]
}